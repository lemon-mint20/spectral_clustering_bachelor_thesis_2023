{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lemon-mint20/Bachelorarbeit/blob/master/Bachelorarbeit_v5_2_Experiment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Spectral Clustering – eine empirische Untersuchung** \n",
    "\n",
    "# Implementierung für Experiment 2\n",
    "\n",
    "\n",
    "## Freie wissenschaftliche Arbeit zur Erlangung des akademischen Grades Bachelor of Science\"\n",
    "\n",
    "Studiengang: Wirtschaftsinformatik\n",
    "\n",
    "**an der Wirtschaftswissenschaftlichen Fakultät der Universität Augsburg**\n",
    "\n",
    "Lehrstuhl für Statistik\n",
    "\n",
    "Eingereicht bei: Prof. Dr. Yarema Okhrin\n",
    "\n",
    "Betreuerin:      Christine Distler (M. Sc.)\n",
    "\n",
    "Vorgelegt von:\n",
    "\n",
    "Adresse:         \n",
    ">\n",
    ">\n",
    ">\n",
    "\n",
    "\n",
    "\n",
    "Augsburg, im März 2023"
   ],
   "metadata": {
    "id": "cwxrvwj0mbPO"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Jm2QRPGg_pO"
   },
   "source": [
    "# Wine Data Set\n",
    "@misc{Dua:2019 ,\n",
    "author = \"Dua, Dheeru and Graff, Casey\",\n",
    "year = \"2017\",\n",
    "title = \"{UCI} Machine Learning Repository\",\n",
    "url = \"http://archive.ics.uci.edu/ml\",\n",
    "institution = \"University of California, Irvine, School of Information and Computer Sciences\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Up98yH15hoUO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn.datasets import load_wine\n",
    "df, ground_truth = load_wine(return_X_y=True, as_frame=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7byd1V3-ievY"
   },
   "source": [
    "## Datenvorbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWPLnLfuTbi5"
   },
   "outputs": [],
   "source": [
    "display(df.info())\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5isDOd3jFbM"
   },
   "outputs": [],
   "source": [
    "# The variables are measured in different units, so I rescale them to the same range\n",
    "# standardize columns with min-max scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z1kOWcTATbi5"
   },
   "outputs": [],
   "source": [
    "display(df_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyVQfqZe32_5"
   },
   "source": [
    "**veränderten Datensatz abspeichern, falls noch nicht vorhanden**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "An_mlEEZ3Tv6"
   },
   "outputs": [],
   "source": [
    "# Speichere den veränderten Datensatz ab\n",
    "# save Dataframe as csv in Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('drive')\n",
    "# Write the DataFrame to CSV file.\n",
    "# with open('/content/drive/My Drive/Colab Notebooks/Bachelorarbeit/wine-clustering_final.csv', 'w') as f:\n",
    "#    df_scaled.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WlvnCRbbTbi6"
   },
   "outputs": [],
   "source": [
    "#save df_final as csv in local directory\n",
    "# df_scaled.to_csv('wine-clustering_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBf24HJojfuF"
   },
   "source": [
    "## Descriptive Statistik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LeTrLohbjhDF"
   },
   "outputs": [],
   "source": [
    "display(df_scaled.describe())\n",
    "df_scaled.boxplot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l0W2GchVkBE6"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(df_scaled, diag_kind='kde', corner=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "heY2SPjvD7Nl"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBT0oqpPWeHN"
   },
   "outputs": [],
   "source": [
    "corr = df_scaled.corr()\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(corr, cmap='coolwarm', annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NM0qoiAgkM68"
   },
   "source": [
    "## Datensatz visualisieren "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_PkmdtaukJVa"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n",
    "Zugriff: 05.01.2023\n",
    "\"\"\"\n",
    "\n",
    "# Standardize the Data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separating out the features\n",
    "x = df_scaled.loc[:, :].values\n",
    "\n",
    "# Standardizing the features\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "\n",
    "# PCA Projection to 2D\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents, \n",
    "                           columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "\n",
    "# Visualize 2D Projection\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.tick_params(labelsize=14)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "ax.scatter(principalDf['principal component 1'], principalDf['principal component 2'])\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QP6TSK0oZXjU"
   },
   "source": [
    "# Gridsearch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzlb-GKypSPx"
   },
   "source": [
    "## Ähnlichkeiten berechnen\n",
    "Für die variable $\\epsilon$ untersuche die Distanzen im Datensatz. Es muss sichergestellt werden, dass der $\\epsilon$-Nachbarschaftsgraph verbunden ist. Die Distanzen können bei sich bei unterschiedlichen Distanzmaßen unterscheiden. Ich untersuche die Distanzen bei Distanzen 'euclidean' und 'correlation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6K6cWsmpgNBJ"
   },
   "source": [
    "### Korrelations Distanz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T20RI9FeTbi8"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8P7B_XRogMvJ"
   },
   "outputs": [],
   "source": [
    "distance = pdist(df_scaled, metric='correlation')\n",
    "flatten_array_corr = np.array(distance).flatten()\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.hist(flatten_array_corr, bins=50)\n",
    "plt.title(\"Korrelations Distanz\", fontsize = 20)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.show()\n",
    "df_dist_corr = pd.DataFrame(flatten_array_corr)\n",
    "display(df_dist_corr.describe())\n",
    "print(\"unique distances and their counts: \")\n",
    "display(np.unique(flatten_array_corr, return_counts=True))\n",
    "print(f\"number of unique distances: {len(np.unique(flatten_array_corr))}\")\n",
    "print(\"distance matrix: \")\n",
    "display(distance)\n",
    "print(distance.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBMl7-osa4v9"
   },
   "source": [
    "### Euklidische Distanz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZ2R3s4Oa4EC"
   },
   "outputs": [],
   "source": [
    "distance = pdist(df_scaled, metric='euclidean')\n",
    "flatten_array_eucl = np.array(distance).flatten()\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.hist(flatten_array_eucl, bins=50)\n",
    "plt.title(\"Euklidische Distanz\", fontsize = 20)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.show()\n",
    "\n",
    "df_dist_eucl = pd.DataFrame(flatten_array_corr)\n",
    "display(df_dist_eucl.describe())\n",
    "print(\"unique distances and their counts: \")\n",
    "display(np.unique(flatten_array_eucl, return_counts=True))\n",
    "print(f\"number of unique distances: {len(np.unique(flatten_array_eucl))}\")\n",
    "print(\"distance matrix: \")\n",
    "display(distance)\n",
    "print(distance.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AHPCiiviYmn"
   },
   "source": [
    "### $sigma$-Grenzen finden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZQuG8GLilEu"
   },
   "outputs": [],
   "source": [
    "# für euklidische Distanz\n",
    "def gaussian(mat, sigma):\n",
    "    return np.exp(-np.square(mat) / (2 * sigma)**2)\n",
    "dist_eucl_unique = np.unique(flatten_array_eucl)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(dist_eucl_unique, gaussian(dist_eucl_unique, 1))\n",
    "plt.scatter(dist_eucl_unique, gaussian(dist_eucl_unique, 0.2))\n",
    "plt.xlabel('Euklidische Distanz', fontsize=12)\n",
    "plt.ylabel('Gaußsche Ähnlichkeitsfunktion', fontsize=12)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.legend(['sigma=1', 'sigma=0.2'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KtZAbz4ykVga"
   },
   "outputs": [],
   "source": [
    "# für corr\n",
    "dist_corr_unique = np.unique(flatten_array_corr)\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(dist_corr_unique, gaussian(dist_corr_unique, 1))\n",
    "plt.scatter(dist_corr_unique, gaussian(dist_corr_unique, 0.2))\n",
    "plt.xlabel('Korrelationsdistanz', fontsize=12)\n",
    "plt.ylabel('Gaußsche Ähnlichkeitsfunktion', fontsize=12)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.legend(['sigma=1', 'sigma=0.2'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqoeQbBW-LNl"
   },
   "source": [
    "### optimale Clusterzahl schätzen (Eigengap Heuristik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bqlznz4n-_SU"
   },
   "outputs": [],
   "source": [
    "sigma = 0.5\n",
    "Adj_mat = squareform(pdist(df_scaled, metric='euclidean'))\n",
    "W = np.exp(-np.square(Adj_mat) / (2 * sigma)**2)\n",
    "D = np.diag(np.sum(W, axis=1))\n",
    "L = D - W\n",
    "D_inv = np.diag(1 / np.diag(D))\n",
    "L_rw = np.dot(D_inv, L)\n",
    "print(L_rw.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YggvuLaf_M7p"
   },
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(np.dot(D_inv, L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kyrM6FT_P2d"
   },
   "outputs": [],
   "source": [
    "# print(np.argsort(np.diff(eigenvalues)))\n",
    "# print(f'Eigenvalues: {eigenvalues}')\n",
    "def maxDiffIndex(array): \n",
    "  array = np.sort(array) \n",
    "  max_diff_index = 0\n",
    "  max_diff = 0\n",
    "  \n",
    "  for i in range(1, len(array)): \n",
    "    diff = array[i] - array[i-1]\n",
    "    if diff > max_diff:\n",
    "      max_diff_index = i\n",
    "      max_diff = diff\n",
    "  \n",
    "  return max_diff_index, max_diff\n",
    "print(maxDiffIndex(eigenvalues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "myuCEGOl_UNj"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.scatter([i for i in range(len(eigenvalues))], np.sort(eigenvalues))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "halGFQde-LAQ"
   },
   "source": [
    "### Parameter-Grid initialisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPqf2PIPwY88"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "params1 = {\n",
    "    'K': [3],\n",
    "    # more metric arguments from scipy.spatial.distance.pdist\n",
    "    'metric': ['euclidean', 'correlation'],\n",
    "    'sim_graph': ['fully_connect'],\n",
    "    'normalized': [1, 2, 3],\n",
    "    'sigma': np.linspace(0.2, 1, 9),\n",
    "}\n",
    "\n",
    "grid1 = list(ParameterGrid(params1))\n",
    "print(len(grid1))\n",
    "\n",
    "params2_1 = {\n",
    "    'K': [3],\n",
    "    # more metric arguments from scipy.spatial.distance.pdist\n",
    "    'metric': ['euclidean'],\n",
    "    'sim_graph': ['eps_neighbor'],\n",
    "    'normalized': [1, 2, 3],\n",
    "    'epsi': np.linspace(df_dist_eucl.quantile(0.20), df_dist_eucl.quantile(0.8), 40),\n",
    "}\n",
    "\n",
    "grid2_1 = list(ParameterGrid(params2_1))\n",
    "print(len(grid2_1))\n",
    "\n",
    "params2_2 = {\n",
    "    'K': [3],\n",
    "    # more metric arguments from scipy.spatial.distance.pdist\n",
    "    'metric': ['correlation'],\n",
    "    'sim_graph': ['eps_neighbor'],\n",
    "    'normalized': [1, 2, 3],\n",
    "    'epsi': np.linspace(df_dist_corr.quantile(0.20), df_dist_corr.quantile(0.8), 20),\n",
    "}\n",
    "\n",
    "grid2_2 = list(ParameterGrid(params2_2))\n",
    "print(len(grid2_2))\n",
    "\n",
    "params3 = {\n",
    "    'K': [3],\n",
    "    # more metric arguments from scipy.spatial.distance.pdist\n",
    "    'metric': ['euclidean', 'correlation'],\n",
    "    'sim_graph': ['knn'],\n",
    "    'normalized': [1, 2, 3],\n",
    "    'knn': np.arange(3, 20),\n",
    "}\n",
    "\n",
    "grid3 = list(ParameterGrid(params3))\n",
    "print(len(grid3))\n",
    "\n",
    "params4 = {\n",
    "    'K': [3],\n",
    "    # more metric arguments from scipy.spatial.distance.pdist\n",
    "    'metric': ['euclidean', 'correlation'],\n",
    "    'sim_graph': ['mutual_knn'],\n",
    "    'normalized': [1, 2, 3],\n",
    "    'knn': np.arange(3, 20),\n",
    "}\n",
    "\n",
    "grid4 = list(ParameterGrid(params4))\n",
    "print(len(grid4))\n",
    "\n",
    "grid = grid1+grid2_1+grid2_2+grid3+grid4\n",
    "print(len(grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rp7rc_UDgJsc"
   },
   "source": [
    "\n",
    "# Implementierung des Spectral Clustering von Dr. Yikun Zhang \n",
    "\n",
    "Quelle: [https://github.com/zhangyk8/Spectral-Clustering/blob/master/spectral_clustering.py](https://github.com/zhangyk8/Spectral-Clustering/blob/master/spectral_clustering.py)\n",
    "\n",
    "(Zugriff: 15.12.2023)\n",
    "\n",
    "Die Berechnung für den vollverbundenen Graph wurde von mir abgeändert zu: \n",
    "```\n",
    "W = np.exp(-np.square(Adj_mat) / (2 * sigma)**2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O0RqNp2tEQVV"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Based on \"A Tutorial on Spectral Clustering\" written by Ulrike von Luxburg\n",
    "def Spectral_Clustering(X, K=8, adj=True, metric='euclidean', sim_graph='fully_connect', sigma=1.0, knn=10, epsi=0.5,\n",
    "                        normalized=1):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        X : [n_samples, n_samples] numpy array if adj=True, or, a [n_samples_a, n_features] array otherwise;\n",
    "\n",
    "        K: int, The number of clusters;\n",
    "\n",
    "        adj: boolean, Indicating whether the adjacency matrix is pre-computed. Default: True;\n",
    "\n",
    "        metric: string, A parameter passing to \"scipy.spatial.distance.pdist()\" function for computing the adjacency\n",
    "        matrix (deprecated if adj=True). Default: 'euclidean';\n",
    "\n",
    "        sim_graph: string, Specifying the type of similarity graphs. Choices are ['fully_connect', 'eps_neighbor',\n",
    "        'knn', 'mutual_knn']. Default: 'fully_connect';\n",
    "\n",
    "        sigma: float, The variance for the Gaussian (aka RBF) kernel (Used when sim_graph='fully_connect'). Default: 1;\n",
    "\n",
    "        knn: int, The number of neighbors used to construct k-Nearest Neighbor graphs (Used when sim_graph='knn'\n",
    "        or 'mutual_knn'). Default: 10;\n",
    "\n",
    "        epsi: float, A parameter controlling the connections between points (Used when sim_graph='eps_neighbor').\n",
    "        Default: 0.5;\n",
    "\n",
    "        normalized: int, 1: Random Walk normalized version; 2: Graph cut normalized version; other integer values:\n",
    "        Unnormalized version. Default: 1.\n",
    "\n",
    "    Output:\n",
    "        sklearn.cluster class, Attributes:\n",
    "            cluster_centers_ : array, [n_clusters, n_features], Coordinates of cluster centers in K-means;\n",
    "            labels_ : Labels of each point;\n",
    "            inertia_ : float, Sum of squared distances of samples to their closest cluster center in K-means;\n",
    "            n_iter_ : int, Number of iterations run in K-means.\n",
    "    \"\"\"\n",
    "    # Compute the adjacency matrix\n",
    "    if not adj:\n",
    "      Adj_mat = squareform(pdist(X, metric=metric))\n",
    "    else:\n",
    "        Adj_mat = X\n",
    "    # Compute the weighted adjacency matrix based on the type of similarity graphs\n",
    "    if sim_graph == 'fully_connect':\n",
    "        W = np.exp(-np.square(Adj_mat) / (2 * sigma)**2)\n",
    "    elif sim_graph == 'eps_neighbor':\n",
    "        W = (Adj_mat <= epsi).astype('float64')\n",
    "    elif sim_graph == 'knn':\n",
    "        W = np.zeros(Adj_mat.shape)\n",
    "        # Sort the adjacency matrx by rows and record the indices\n",
    "        Adj_sort = np.argsort(Adj_mat, axis=1)\n",
    "        # Set the weight (i,j) to 1 when either i or j is within the k-nearest neighbors of each other\n",
    "        for i in range(Adj_sort.shape[0]):\n",
    "            W[i, Adj_sort[i, :][:(knn + 1)]] = 1\n",
    "    elif sim_graph == 'mutual_knn':\n",
    "        W1 = np.zeros(Adj_mat.shape)\n",
    "        # Sort the adjacency matrx by rows and record the indices\n",
    "        Adj_sort = np.argsort(Adj_mat, axis=1)\n",
    "        # Set the weight W1[i,j] to 0.5 when either i or j is within the k-nearest neighbors of each other (Flag)\n",
    "        # Set the weight W1[i,j] to 1 when both i and j are within the k-nearest neighbors of each other\n",
    "        for i in range(Adj_mat.shape[0]):\n",
    "            for j in Adj_sort[i, :][:(knn + 1)]:\n",
    "                if i == j:\n",
    "                    W1[i, i] = 1\n",
    "                elif W1[i, j] == 0 and W1[j, i] == 0:\n",
    "                    W1[i, j] = 0.5\n",
    "                else:\n",
    "                    W1[i, j] = W1[j, i] = 1\n",
    "        W = np.copy((W1 > 0.5).astype('float64'))\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"The 'sim_graph' argument should be one of the strings, 'fully_connect', 'eps_neighbor', 'knn', or 'mutual_knn'!\")\n",
    "\n",
    "    # Compute the degree matrix and the unnormalized graph Laplacian\n",
    "    D = np.diag(np.sum(W, axis=1))\n",
    "    L = D - W\n",
    "\n",
    "    # Compute the matrix with the first K eigenvectors as columns based on the normalized type of L\n",
    "    if normalized == 1:  ## Random Walk normalized version\n",
    "        # Compute the inverse of the diagonal matrix\n",
    "        D_inv = np.diag(1 / np.diag(D))\n",
    "        # Compute the eigenpairs of L_{rw}\n",
    "        Lambdas, V = np.linalg.eig(np.dot(D_inv, L))\n",
    "        # Sort the eigenvalues by their L2 norms and record the indices\n",
    "        ind = np.argsort(np.linalg.norm(np.reshape(Lambdas, (1, len(Lambdas))), axis=0))\n",
    "        V_K = np.real(V[:, ind[:K]])\n",
    "    elif normalized == 2:  ## Graph cut normalized version\n",
    "        # Compute the square root of the inverse of the diagonal matrix\n",
    "        D_inv_sqrt = np.diag(1 / np.sqrt(np.diag(D)))\n",
    "        # Compute the eigenpairs of L_{sym}\n",
    "        Lambdas, V = np.linalg.eig(np.matmul(np.matmul(D_inv_sqrt, L), D_inv_sqrt))\n",
    "        # Sort the eigenvalues by their L2 norms and record the indices\n",
    "        ind = np.argsort(np.linalg.norm(np.reshape(Lambdas, (1, len(Lambdas))), axis=0))\n",
    "        V_K = np.real(V[:, ind[:K]])\n",
    "        if any(V_K.sum(axis=1) == 0):\n",
    "            raise ValueError(\n",
    "                \"Can't normalize the matrix with the first K eigenvectors as columns! Perhaps the number of clusters K or the number of neighbors in k-NN is too small.\")\n",
    "        # Normalize the row sums to have norm 1\n",
    "        V_K = V_K / np.reshape(np.linalg.norm(V_K, axis=1), (V_K.shape[0], 1))\n",
    "    else:  ## Unnormalized version\n",
    "        # Compute the eigenpairs of L\n",
    "        Lambdas, V = np.linalg.eig(L)\n",
    "        # Sort the eigenvalues by their L2 norms and record the indices\n",
    "        ind = np.argsort(np.linalg.norm(np.reshape(Lambdas, (1, len(Lambdas))), axis=0))\n",
    "        V_K = np.real(V[:, ind[:K]])\n",
    "\n",
    "    # Conduct K-Means on the matrix with the first K eigenvectors as columns\n",
    "    kmeans = KMeans(n_clusters=K, init='k-means++', random_state=0).fit(V_K)\n",
    "    return kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UDqRVACt1F3"
   },
   "source": [
    "# Spectral clustering auf alle Datensätze anwenden# run spectral clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLQW32fY1eTb"
   },
   "outputs": [],
   "source": [
    "import time as time\n",
    "import warnings\n",
    "from sklearn.metrics import adjusted_rand_score, silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "warnings.filterwarnings(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRfrCkIX19YA"
   },
   "source": [
    "Im selbstgenerierten Datensatz sind die ground truth classes bekannt. Daher Werte ich das Clustering mit dem ARI aus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W5Eqm2tE1fJ2"
   },
   "outputs": [],
   "source": [
    "def evaluate_single_dataset(dataset):\n",
    "  silhouette_scores = []\n",
    "  calinski_h_scores = []\n",
    "  davies_b_scores = []\n",
    "  ari = []\n",
    "\n",
    "  has_exception = []\n",
    "  iter_with_LinAlgError = []\n",
    "  iter_with_ValueError = []\n",
    "  iter_with_RuntimeWarning = []\n",
    "  iter_with_Errors_Or_Warnings = []\n",
    "  iter_without_Errors = []\n",
    "  # eigenvalues_of_one_dataset = []\n",
    "  \n",
    "  dataset_index = []\n",
    "  metric = []\n",
    "  simgraph = []\n",
    "  sigma = []\n",
    "  knn = []\n",
    "  epsi = []\n",
    "  normalized = []\n",
    "\n",
    "  for i in range(len(grid)):\n",
    "    try:\n",
    "      clust = Spectral_Clustering(X=dataset, **grid[i], adj=False)\n",
    "    except np.linalg.LinAlgError:\n",
    "      iter_with_LinAlgError.append(i)\n",
    "      has_exception.append(1)\n",
    "      continue\n",
    "    except ValueError as e:\n",
    "      # print(e)\n",
    "      iter_with_ValueError.append(i)\n",
    "      has_exception.append(1)\n",
    "      continue\n",
    "    except RuntimeWarning as e: \n",
    "      # print(e)\n",
    "      iter_with_RuntimeWarning.append(i)\n",
    "      has_exception.append(1)\n",
    "      continue\n",
    "    except:\n",
    "      iter_with_Errors_Or_Warnings.append(i)\n",
    "      has_exception.append(1)\n",
    "      continue\n",
    "    else:\n",
    "      dataset_index.append(i)\n",
    "      has_exception.append(0)\n",
    "      metric.append(grid[i]['metric'])\n",
    "      simgraph.append(grid[i]['sim_graph'])\n",
    "      if 'knn' in grid[i]:\n",
    "        knn.append(grid[i]['knn'])\n",
    "        sigma.append(-1)\n",
    "        epsi.append(-1)\n",
    "      if 'sigma' in grid[i]:\n",
    "        sigma.append(grid[i]['sigma'])\n",
    "        knn.append(-1)\n",
    "        epsi.append(-1)\n",
    "      if 'epsi' in grid[i]:\n",
    "        epsi.append(grid[i]['epsi'])\n",
    "        knn.append(-1)\n",
    "        sigma.append(-1)\n",
    "      normalized.append(grid[i]['normalized'])\n",
    "      ari.append(adjusted_rand_score(clust.labels_, ground_truth))\n",
    "\n",
    "      iter_without_Errors.append(i)\n",
    "      score1 = silhouette_score(dataset, clust.labels_)\n",
    "      score2 = calinski_harabasz_score(dataset, clust.labels_)\n",
    "      score3 = davies_bouldin_score(dataset, clust.labels_)\n",
    "      silhouette_scores.append(score1)\n",
    "      calinski_h_scores.append(score2)\n",
    "      davies_b_scores.append(score3)\n",
    "      # eigenvalues_of_one_dataset.append(eigenvalues)\n",
    "\n",
    "      # print(f'\\t Iteration: {i}')\n",
    "      # print(f'silhouette:         {score1}')\n",
    "      # print(f'calinski_harabasz:  {score2}')\n",
    "      # print(f'davies_bouldin: \n",
    "      \n",
    "  return (silhouette_scores, \n",
    "          calinski_h_scores, \n",
    "          davies_b_scores, \n",
    "\n",
    "          metric,\n",
    "          simgraph,\n",
    "          sigma,\n",
    "          knn,\n",
    "          epsi, \n",
    "          normalized,\n",
    "          \n",
    "          iter_with_LinAlgError, \n",
    "          iter_with_ValueError, \n",
    "          iter_with_RuntimeWarning, \n",
    "          iter_with_Errors_Or_Warnings, \n",
    "          iter_without_Errors, \n",
    "          # eigenvalues_of_one_dataset, \n",
    "          dataset_index,\n",
    "          ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekEuezvf1jiB"
   },
   "outputs": [],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "(silhouette_scores, \n",
    "calinski_h_scores, \n",
    "davies_b_scores, \n",
    "metric, \n",
    "simgraph,\n",
    "sigma, \n",
    "knn, \n",
    "epsi, \n",
    "normalized, \n",
    "iter_with_LinAlgError, \n",
    "iter_with_ValueError, \n",
    "iter_with_RuntimeWarning, \n",
    "iter_with_Errors_Or_Warnings, \n",
    "iter_without_Errors, \n",
    "# eigenvalues_of_one_dataset, \n",
    "dataset_index, \n",
    "ari) = evaluate_single_dataset(df_scaled)\n",
    "\n",
    "executionTime = (time.time() - startTime)\n",
    "print('Execution time in seconds: ' + str(executionTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QWZz_sANr_M"
   },
   "source": [
    "# Datensatz zur Auswertung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4n42NDyNyZY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3n3r0yXYN5LQ"
   },
   "outputs": [],
   "source": [
    "df_experiment2 = pd.DataFrame(data={\n",
    "        'Dataset': dataset_index,\n",
    "        'Iteration': iter_without_Errors,\n",
    "        'metric': metric,\n",
    "        'sim_graph': simgraph,\n",
    "        'sigma': sigma,\n",
    "        'knn': knn,\n",
    "        'epsi': np.hstack(epsi),\n",
    "        'normalised': normalized,\n",
    "        'Calinski': np.hstack(calinski_h_scores),\n",
    "        'Davis': np.hstack(davies_b_scores),\n",
    "        'Silhouette': np.hstack(silhouette_scores),\n",
    "        'ari': ari,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gctu8c9QSK5a"
   },
   "outputs": [],
   "source": [
    "df_experiment2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzzJHVCcOJ18"
   },
   "outputs": [],
   "source": [
    "# save Dataframe as csv in Google Drive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('drive')\n",
    "# Write the DataFrame to CSV file.\n",
    "with open('/content/drive/My Drive/Colab Notebooks/Bachelorarbeit/Experiment2.csv', 'w') as f:\n",
    "  df_experiment2.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nVrSP9MqUl5K"
   },
   "outputs": [],
   "source": [
    "df_experiment2.to_csv('Experiment2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5zHVSmyN7aU"
   },
   "source": [
    "# Datensatz importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WkOrRJ_JJWOo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# from google.colab import drive\n",
    "# drive.mount('drive')\n",
    "# df_experiment2 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Bachelorarbeit/Experiment2.csv')\n",
    "df_experiment2 = pd.read_csv('Experiment2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3ujtrB-JUSR"
   },
   "outputs": [],
   "source": [
    "# Datensatz auf NaN-Werte überprüfen\n",
    "display(df_experiment2.info())\n",
    "df_experiment2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1etmn-FXlAU"
   },
   "outputs": [],
   "source": [
    "df_experiment2 = df_experiment2.drop(['Unnamed: 0', 'Dataset'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ARI-Werte untersuchen"
   ],
   "metadata": {
    "id": "BTlhyvmuorbT"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyYlGOM8hiHa"
   },
   "outputs": [],
   "source": [
    "# Verteilung der ARI-Werte untersuchen\n",
    "# print(pd.unique(df.ari))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(df_experiment2.ari, bins=len(pd.unique(df_experiment2.ari)))\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UPSApq-P2pr"
   },
   "outputs": [],
   "source": [
    "ari_rounded = df_experiment2.ari.round(1)\n",
    "ari, counts = np.unique(ari_rounded, return_counts=True)\n",
    "print(ari, counts)\n",
    "print(counts/(len(ari_rounded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "keHTu59TRfAT"
   },
   "outputs": [],
   "source": [
    "df_experiment2.ari.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A50Jb3sIlV8v"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20.0, 4.0))\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.title('Adjusted Rand Index')\n",
    "aris = df_experiment2.ari\n",
    "plt.plot(aris, 'bo', aris, 'k')\n",
    "# make more xticks\n",
    "plt.xticks(np.arange(0, len(aris), 20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ARI für jede Iteration geplottet"
   ],
   "metadata": {
    "id": "AdDNecxZoyhH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MmGCtds4gs-I"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20.0, 10.0))\n",
    "\n",
    "plt.subplot(311)\n",
    "plt.title('Silhouettenkoeffizient')\n",
    "plt.plot(df_experiment2.Silhouette, 'bo', df_experiment2.Silhouette, 'k')\n",
    "plt.xticks(np.arange(0, len(aris), 20))\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.title('Calinski-Harabasz Index')\n",
    "plt.plot(df_experiment2.Calinski, 'bo', df_experiment2.Calinski, 'k')\n",
    "plt.xticks(np.arange(0, len(aris), 20))\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.title('Davies-Bouldin Index')\n",
    "plt.plot(df_experiment2.Davis, 'bo', df_experiment2.Davis, 'k')\n",
    "plt.xticks(np.arange(0, len(aris), 20))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpGGg_fOkugx"
   },
   "source": [
    "# Clusteringergebnisse untersuchen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ci5GKRI3acfd"
   },
   "source": [
    "## *1.* Untersuchung der Metrik auf die Clusterqualität"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GlMf41qnoiT1"
   },
   "outputs": [],
   "source": [
    "df_experiment2.groupby('metric').ari.describe().round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StKDfiluoiT2"
   },
   "source": [
    "## *2.* Einfluss des Ähnlichkeitsgraphen auf die Clusterqualität"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUbcC2WSoiT2"
   },
   "outputs": [],
   "source": [
    "df_experiment2.groupby('sim_graph').ari.describe().round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_u7RPwPvoiT3"
   },
   "source": [
    "## *3.* Einfluss von $\\sigma$ auf die Clusterqualität\n",
    "Der Einfluss soll mit der Korrelation berechnet werden. Um einzuschätzen, welches Korrelationsmaß verwendet werden soll und damit ich eine Einschätzung über die Richtung der Korrelation habe, plotte ich sigma und die Clustergüte in einem Scatterplot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7H7HQwjRfAY"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5V2xxL7MoiT3"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.scatterplot(data=df_experiment2[df_experiment2.sigma != -1], x='sigma', y='ari')\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YK789m5JoiT4"
   },
   "outputs": [],
   "source": [
    "df_experiment2[df_experiment2.sigma != -1].sigma.corr(df_experiment2[df_experiment2.sigma != -1].ari, method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wk-6CmRBBK0A",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Was sind die häufigsten Werte? \n",
    "ari_round_unique = np.unique(np.round(df_experiment2[df_experiment2.sigma != 1].ari,1), return_counts=True)\n",
    "# sortiere ari_round nach Häufigkeit\n",
    "ari_round_unique_sorted = np.array([x for _,x in sorted(zip(ari_round_unique[1],ari_round_unique[0]), reverse=True)])\n",
    "ari_round_unique_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BCCznUj29hnY",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df2 = df_experiment2[df_experiment2.sigma != -1].copy()\n",
    "df2.ari = np.round(df2.ari, 1)\n",
    "df3 = df2[df2.ari.isin(ari_round_unique_sorted[:4])].copy()\n",
    "# df3.groupby(['ari', 'metric']).sigma.describe()\n",
    "# df3.groupby(['sigma']).ari.describe()\n",
    "df3.groupby(['ari']).sigma.describe().round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2OHYbG5oiT4"
   },
   "source": [
    "## *4.1* Einfluss von $k$ beim knn-Graphen auf die Clusterqualität"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67dM2emroiT5"
   },
   "outputs": [],
   "source": [
    "# plotte die Spalte k, bei dem der sim-graph knn ist und die Clustergüte in einem Scatterplot\n",
    "plt.figure(figsize=(7, 7))\n",
    "sns.scatterplot(data=df_experiment2[df_experiment2.sim_graph == 'knn'], x='knn', y='ari')\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0q5rCWhRfAa"
   },
   "source": [
    "## *4.2* Einfluss von $k$ beim mutual-knn-Graphen auf die Clusterqualität"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yw_uMWrbRfAa"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "sns.scatterplot(data=df_experiment2[df_experiment2.sim_graph == 'mutual_knn'], x='knn', y='ari')\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FfSwykqiduPp"
   },
   "outputs": [],
   "source": [
    "# Wie viele Iterationen wurden insgesammt mit dem mutual-knn erstellt.\n",
    "df_experiment2[df_experiment2.sim_graph == 'mutual_knn'] \n",
    "# df[(df.sim_graph == 'mutual_knn') & (df.ari != 1.0) & (df.ari != 0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZ6Mu2bRxXic"
   },
   "outputs": [],
   "source": [
    "df_experiment2[(df_experiment2.normalised == 2) & (df_experiment2.sim_graph == 'mutual_knn')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KH7FXvmZoiT5"
   },
   "source": [
    "## *5.* Einfluss von $\\epsilon$ auf die Clusterqualität"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8izUu0uNoiT5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.scatterplot(data=df_experiment2[df_experiment2.sim_graph == 'eps_neighbor'], x='epsi', y='ari')\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6P-aJFNu0L4E"
   },
   "outputs": [],
   "source": [
    "df_experiment2[df_experiment2.sim_graph == 'eps_neighbor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksP31AwaRfAb"
   },
   "source": [
    "## *6.* Einfluss vom Laplace-Graphen auf die Clusterqualität"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g4Egs6YzoiT6",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_experiment2.groupby('normalised').ari.describe().round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNNPnPymMiZy"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(data=df_experiment2, x='normalised', y='ari')\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Zusammenfassung"
   ],
   "metadata": {
    "collapsed": false,
    "id": "NjXtaMljmObS"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_experiment2.head()"
   ],
   "metadata": {
    "id": "FBjyCL4FmObS"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = df_experiment2.groupby(['metric']).ari.describe().round(4)\n",
    "b = df_experiment2.groupby(['sim_graph']).ari.describe().round(4)\n",
    "c = df_experiment2[df_experiment2.sim_graph == 'fully_connect'].groupby(['metric']).ari.describe().round(4)\n",
    "c.index = c.index + '_fully_connect'\n",
    "d = df_experiment2.groupby(['normalised']).ari.describe().round(4)\n",
    "\n",
    "frames = [a,b,c,d]\n",
    "summary = pd.concat(frames)\n",
    "summary = summary.rename({\"metric\":\"parameter\"})"
   ],
   "metadata": {
    "id": "syNDQvcumObT"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "summary.sort_values(by=['mean','std'], ascending=[False, True])[['mean', 'std']]"
   ],
   "metadata": {
    "id": "5-hTt_mlmObT"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_experiment2.groupby(['sim_graph', 'metric', 'normalised']).ari.describe().round(4).sort_values(by=['mean','std'], ascending=[False, True])[['mean', 'std']]"
   ],
   "metadata": {
    "id": "R0x3seegmObU"
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
