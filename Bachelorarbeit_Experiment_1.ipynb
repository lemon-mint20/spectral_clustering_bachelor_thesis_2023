{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lemon-mint20/Bachelorarbeit/blob/master/Bachelorarbeit_v5_2_Experiment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Spectral Clustering – eine empirische Untersuchung** \n",
    "\n",
    "# Implementierung für Experiment 1\n",
    "\n",
    "\n",
    "## Freie wissenschaftliche Arbeit zur Erlangung des akademischen Grades \"Bachelor of Science\"\n",
    "\n",
    "Studiengang: Wirtschaftsinformatik\n",
    "\n",
    "**an der Wirtschaftswissenschaftlichen Fakultät der Universität Augsburg**\n",
    "\n",
    "Lehrstuhl für Statistik\n",
    "\n",
    "Eingereicht bei: Prof. Dr. Yarema Okhrin\n",
    "\n",
    "Betreuerin:      Christine Distler (M. Sc.)\n",
    "\n",
    "Vorgelegt von:\n",
    "\n",
    "Adresse:         \n",
    ">\n",
    ">\n",
    ">\n",
    "\n",
    "\n",
    "\n",
    "Augsburg, im März 2023\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "hdkkGvSRuwcx"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Erstellung der synthetischen Datensätze"
   ],
   "metadata": {
    "id": "MaxxuWmWExml"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "N_SAMPLES = 300\n",
    "NOISE_BLOOBS = [0.07, 0.09, 0.11]\n",
    "NOISE_CIRCLES = [0.07, 0.09, 0.11]\n",
    "NOISE_MOONS = [0.05, 0.09, 0.13]"
   ],
   "metadata": {
    "id": "-hca0P8SYgOT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "generated_datasets = []\n",
    "ground_truth = []\n",
    "X, y = datasets.make_blobs(n_samples=N_SAMPLES, centers=2, \n",
    "                             cluster_std=NOISE_BLOOBS[0], \n",
    "                             random_state=0, center_box=(-1.5, 1.5))\n",
    "generated_datasets.append(X)\n",
    "ground_truth.append(y)\n",
    "X, y = datasets.make_circles(n_samples=N_SAMPLES, noise=NOISE_CIRCLES[0], \n",
    "                             factor = 0.4, random_state=1)\n",
    "generated_datasets.append(X)\n",
    "ground_truth.append(y)\n",
    "X, y = datasets.make_moons(n_samples=N_SAMPLES, noise=NOISE_MOONS[0],\n",
    "                             random_state=1)\n",
    "generated_datasets.append(X)\n",
    "ground_truth.append(y)\n",
    "generated_datasets = np.array(generated_datasets)\n",
    "ground_truth = np.array(ground_truth)"
   ],
   "metadata": {
    "id": "rUQMyyLwYO6i"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gridsearch initialisieren\n"
   ],
   "metadata": {
    "id": "QP6TSK0oZXjU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import pandas as pd\n",
    "from IPython.display import display"
   ],
   "metadata": {
    "id": "-FHMGG27vXKG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ähnlichkeiten berechnen\n",
    "Für die variable $\\epsilon$ untersuche die Distanzen im Datensatz. Es muss sichergestellt werden, dass der $\\epsilon$-Nachbarschaftsgraph verbunden ist. Die Distanzen können bei sich bei unterschiedlichen Distanzmaßen unterscheiden. Ich untersuche die Distanzen bei Distanzen 'euclidean' und 'correlation'"
   ],
   "metadata": {
    "id": "pzlb-GKypSPx"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Euklidische Distanz"
   ],
   "metadata": {
    "id": "KLP7gIJXf4Qg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "distance_1 = squareform(pdist(generated_datasets[0], metric='euclidean'))\n",
    "distance_2 = squareform(pdist(generated_datasets[1], metric='euclidean'))\n",
    "distance_3 = squareform(pdist(generated_datasets[2], metric='euclidean'))\n",
    "flatten_array = np.array([distance_1, distance_2, distance_3]).flatten()\n",
    "\n",
    "plt.figure(figsize=(18, 10))\n",
    "plt.hist(flatten_array, bins=500)\n",
    "plt.show()\n",
    "plt.boxplot(flatten_array)\n",
    "plt.show()\n",
    "df_dist_eucl = pd.DataFrame(flatten_array)\n",
    "display(df_dist_eucl.describe())\n",
    "print(distance_1.shape)"
   ],
   "metadata": {
    "id": "ToGFlHetZhVY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Korrelations Distanz"
   ],
   "metadata": {
    "id": "6K6cWsmpgNBJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "distance_1 = squareform(pdist(generated_datasets[0], metric='correlation'))\n",
    "distance_2 = squareform(pdist(generated_datasets[1], metric='correlation'))\n",
    "distance_3 = squareform(pdist(generated_datasets[2], metric='correlation'))\n",
    "flatten_array_corr = np.array([distance_1, distance_2, distance_3]).flatten()\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.hist(flatten_array_corr)\n",
    "plt.show()\n",
    "df_dist_corr = pd.DataFrame(flatten_array_corr)\n",
    "display(df_dist_corr.describe())\n",
    "display(np.unique(flatten_array_corr, return_counts=True))\n",
    "display(distance_1)\n",
    "print(distance_1.shape)"
   ],
   "metadata": {
    "id": "8P7B_XRogMvJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Für die Metrik 'euclidean' wähle ich einen Bereich zwischen dem oberen und unteren Quantil.\n",
    "Bei der Metrik 'correlation' habe ich deutlich weniger Ausprägungen als bei 'euclidean'. Auch die Werte bei Correlation müssen im Experiment berücksichtigt werden. \n",
    "* Als $\\epsilon$ wähle ich Werte zwischen dem unteren und oberen Quantil. Ich erhoffe mir, ein Muster für die Clusterqualität in diesem Bereich zu erkennen\n",
    "* Einen Anhaltspunkt zur Auswahl von $\\sigma$ wird im Paper gegeben. Jedoch ist es nur eine Daumenregel. Ich wähle einen möglichst großen Bereich. "
   ],
   "metadata": {
    "id": "sA8hHJRE2viD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "params = {\n",
    "    'K': [2],\n",
    "    # more metric arguments from scipy.spatial.distance.pdist\n",
    "    'metric': ['euclidean', 'correlation'],\n",
    "    'sim_graph': ['fully_connect', 'eps_neighbor', 'knn', 'mutual_knn'],\n",
    "    'normalized': [1, 2, 3],\n",
    "    'sigma': np.linspace(0.01, 1.5, 30),\n",
    "    'knn': np.arange(2, 10),\n",
    "    'epsi': np.linspace(df_dist_eucl.quantile(0.25), df_dist_eucl.quantile(0.75), 30),\n",
    "}\n",
    "\n",
    "grid = list(ParameterGrid(params))"
   ],
   "metadata": {
    "id": "2wqwmuDHlcnt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "len(grid)"
   ],
   "metadata": {
    "id": "MzojKLqC6gJh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bei diesen vielen Kombinationen, dauert die Berechnung sehr lange und bricht ab. Außerdem werden Kombinationen erstellt, die garnicht gebraucht werden. Wenn z.B. $sim\\_graph = 'fully\\_connect'$, dann sind Kombinationen mit der Variable $epsi$ nicht zu gebrauchen. Daher optimiere ich die Gridsearch"
   ],
   "metadata": {
    "id": "yQk5279SvI7z"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "params1 = {\n",
    "    'K': [2],\n",
    "    # more metric arguments from scipy.spatial.distance.pdist\n",
    "    'metric': ['euclidean', 'correlation'],\n",
    "    'sim_graph': ['fully_connect'],\n",
    "    'normalized': [1, 2, 3],\n",
    "    'sigma': np.linspace(0.01, 1.5, 30),\n",
    "}\n",
    "\n",
    "grid1 = list(ParameterGrid(params1))\n",
    "print(len(grid1))\n",
    "\n",
    "params2_1 = {\n",
    "    'K': [2],\n",
    "    # more metric arguments from scipy.spatial.distance.pdist\n",
    "    'metric': ['euclidean'],\n",
    "    'sim_graph': ['eps_neighbor'],\n",
    "    'normalized': [1, 2, 3],\n",
    "    'epsi': np.linspace(df_dist_eucl.quantile(0.25), df_dist_eucl.quantile(0.75), 30),\n",
    "}\n",
    "\n",
    "grid2_1 = list(ParameterGrid(params2_1))\n",
    "print(len(grid2_1))\n",
    "\n",
    "params2_2 = {\n",
    "    'K': [2],\n",
    "    # more metric arguments from scipy.spatial.distance.pdist\n",
    "    'metric': ['correlation'],\n",
    "    'sim_graph': ['eps_neighbor'],\n",
    "    'normalized': [1, 2, 3],\n",
    "    'epsi': np.unique(flatten_array_corr),\n",
    "}\n",
    "\n",
    "grid2_2 = list(ParameterGrid(params2_2))\n",
    "print(len(grid2_2))\n",
    "\n",
    "params3 = {\n",
    "    'K': [2],\n",
    "    # more metric arguments from scipy.spatial.distance.pdist\n",
    "    'metric': ['euclidean', 'correlation'],\n",
    "    'sim_graph': ['knn'],\n",
    "    'normalized': [1, 2, 3],\n",
    "    'knn': np.arange(3, 20),\n",
    "}\n",
    "\n",
    "grid3 = list(ParameterGrid(params3))\n",
    "print(len(grid3))\n",
    "\n",
    "params4 = {\n",
    "    'K': [2],\n",
    "    # more metric arguments from scipy.spatial.distance.pdist\n",
    "    'metric': ['euclidean', 'correlation'],\n",
    "    'sim_graph': ['mutual_knn'],\n",
    "    'normalized': [1, 2, 3],\n",
    "    'knn': np.arange(3, 20),\n",
    "}\n",
    "\n",
    "grid4 = list(ParameterGrid(params4))\n",
    "print(len(grid4))\n",
    "\n",
    "grid = grid1+grid2_1+grid2_2+grid3+grid4\n",
    "print(len(grid))"
   ],
   "metadata": {
    "id": "qPqf2PIPwY88"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "grid[180]"
   ],
   "metadata": {
    "id": "q-GP-qmZ9vET"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Implementierung des Spectral Clustering von Dr. Yikun Zhang \n",
    "\n",
    "Quelle: [https://github.com/zhangyk8/Spectral-Clustering/blob/master/spectral_clustering.py](https://github.com/zhangyk8/Spectral-Clustering/blob/master/spectral_clustering.py)\n",
    "\n",
    "(Zugriff: 15.12.2023)\n",
    "\n",
    "Die Berechnung für den vollverbundenen Graph wurde von mir abgeändert zu: \n",
    "```\n",
    "W = np.exp(-np.square(Adj_mat) / (2 * sigma)**2)\n",
    "```\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "Xt_TeKZkyRYZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Based on \"A Tutorial on Spectral Clustering\" written by Ulrike von Luxburg\n",
    "def Spectral_Clustering(X, K=8, adj=True, metric='euclidean', sim_graph='fully_connect', sigma=1.0, knn=10, epsi=0.5,\n",
    "                        normalized=1):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        X : [n_samples, n_samples] numpy array if adj=True, or, a [n_samples_a, n_features] array otherwise;\n",
    "\n",
    "        K: int, The number of clusters;\n",
    "\n",
    "        adj: boolean, Indicating whether the adjacency matrix is pre-computed. Default: True;\n",
    "\n",
    "        metric: string, A parameter passing to \"scipy.spatial.distance.pdist()\" function for computing the adjacency\n",
    "        matrix (deprecated if adj=True). Default: 'euclidean';\n",
    "\n",
    "        sim_graph: string, Specifying the type of similarity graphs. Choices are ['fully_connect', 'eps_neighbor',\n",
    "        'knn', 'mutual_knn']. Default: 'fully_connect';\n",
    "\n",
    "        sigma: float, The variance for the Gaussian (aka RBF) kernel (Used when sim_graph='fully_connect'). Default: 1;\n",
    "\n",
    "        knn: int, The number of neighbors used to construct k-Nearest Neighbor graphs (Used when sim_graph='knn'\n",
    "        or 'mutual_knn'). Default: 10;\n",
    "\n",
    "        epsi: float, A parameter controlling the connections between points (Used when sim_graph='eps_neighbor').\n",
    "        Default: 0.5;\n",
    "\n",
    "        normalized: int, 1: Random Walk normalized version; 2: Graph cut normalized version; other integer values:\n",
    "        Unnormalized version. Default: 1.\n",
    "\n",
    "    Output:\n",
    "        sklearn.cluster class, Attributes:\n",
    "            cluster_centers_ : array, [n_clusters, n_features], Coordinates of cluster centers in K-means;\n",
    "            labels_ : Labels of each point;\n",
    "            inertia_ : float, Sum of squared distances of samples to their closest cluster center in K-means;\n",
    "            n_iter_ : int, Number of iterations run in K-means.\n",
    "    \"\"\"\n",
    "    # Compute the adjacency matrix\n",
    "    if not adj:\n",
    "      Adj_mat = squareform(pdist(X, metric=metric))\n",
    "    else:\n",
    "        Adj_mat = X\n",
    "    # Compute the weighted adjacency matrix based on the type of similarity graphs\n",
    "    if sim_graph == 'fully_connect':\n",
    "        W = np.exp(-np.square(Adj_mat) / (2 * sigma)**2)\n",
    "    elif sim_graph == 'eps_neighbor':\n",
    "        W = (Adj_mat <= epsi).astype('float64')\n",
    "    elif sim_graph == 'knn':\n",
    "        W = np.zeros(Adj_mat.shape)\n",
    "        # Sort the adjacency matrx by rows and record the indices\n",
    "        Adj_sort = np.argsort(Adj_mat, axis=1)\n",
    "        # Set the weight (i,j) to 1 when either i or j is within the k-nearest neighbors of each other\n",
    "        for i in range(Adj_sort.shape[0]):\n",
    "            W[i, Adj_sort[i, :][:(knn + 1)]] = 1\n",
    "    elif sim_graph == 'mutual_knn':\n",
    "        W1 = np.zeros(Adj_mat.shape)\n",
    "        # Sort the adjacency matrx by rows and record the indices\n",
    "        Adj_sort = np.argsort(Adj_mat, axis=1)\n",
    "        # Set the weight W1[i,j] to 0.5 when either i or j is within the k-nearest neighbors of each other (Flag)\n",
    "        # Set the weight W1[i,j] to 1 when both i and j are within the k-nearest neighbors of each other\n",
    "        for i in range(Adj_mat.shape[0]):\n",
    "            for j in Adj_sort[i, :][:(knn + 1)]:\n",
    "                if i == j:\n",
    "                    W1[i, i] = 1\n",
    "                elif W1[i, j] == 0 and W1[j, i] == 0:\n",
    "                    W1[i, j] = 0.5\n",
    "                else:\n",
    "                    W1[i, j] = W1[j, i] = 1\n",
    "        W = np.copy((W1 > 0.5).astype('float64'))\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"The 'sim_graph' argument should be one of the strings, 'fully_connect', 'eps_neighbor', 'knn', or 'mutual_knn'!\")\n",
    "\n",
    "    # Compute the degree matrix and the unnormalized graph Laplacian\n",
    "    D = np.diag(np.sum(W, axis=1))\n",
    "    L = D - W\n",
    "\n",
    "    # Compute the matrix with the first K eigenvectors as columns based on the normalized type of L\n",
    "    if normalized == 1:  ## Random Walk normalized version\n",
    "        # Compute the inverse of the diagonal matrix\n",
    "        D_inv = np.diag(1 / np.diag(D))\n",
    "        # Compute the eigenpairs of L_{rw}\n",
    "        Lambdas, V = np.linalg.eig(np.dot(D_inv, L))\n",
    "        # Sort the eigenvalues by their L2 norms and record the indices\n",
    "        ind = np.argsort(np.linalg.norm(np.reshape(Lambdas, (1, len(Lambdas))), axis=0))\n",
    "        V_K = np.real(V[:, ind[:K]])\n",
    "    elif normalized == 2:  ## Graph cut normalized version\n",
    "        # Compute the square root of the inverse of the diagonal matrix\n",
    "        D_inv_sqrt = np.diag(1 / np.sqrt(np.diag(D)))\n",
    "        # Compute the eigenpairs of L_{sym}\n",
    "        Lambdas, V = np.linalg.eig(np.matmul(np.matmul(D_inv_sqrt, L), D_inv_sqrt))\n",
    "        # Sort the eigenvalues by their L2 norms and record the indices\n",
    "        ind = np.argsort(np.linalg.norm(np.reshape(Lambdas, (1, len(Lambdas))), axis=0))\n",
    "        V_K = np.real(V[:, ind[:K]])\n",
    "        if any(V_K.sum(axis=1) == 0):\n",
    "            raise ValueError(\n",
    "                \"Can't normalize the matrix with the first K eigenvectors as columns! Perhaps the number of clusters K or the number of neighbors in k-NN is too small.\")\n",
    "        # Normalize the row sums to have norm 1\n",
    "        V_K = V_K / np.reshape(np.linalg.norm(V_K, axis=1), (V_K.shape[0], 1))\n",
    "    else:  ## Unnormalized version\n",
    "        # Compute the eigenpairs of L\n",
    "        Lambdas, V = np.linalg.eig(L)\n",
    "        # Sort the eigenvalues by their L2 norms and record the indices\n",
    "        ind = np.argsort(np.linalg.norm(np.reshape(Lambdas, (1, len(Lambdas))), axis=0))\n",
    "        V_K = np.real(V[:, ind[:K]])\n",
    "\n",
    "    # Conduct K-Means on the matrix with the first K eigenvectors as columns\n",
    "    kmeans = KMeans(n_clusters=K, init='k-means++', random_state=0).fit(V_K)\n",
    "    return kmeans"
   ],
   "metadata": {
    "id": "O0RqNp2tEQVV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Spectral clustering auf alle Datensätze anwenden"
   ],
   "metadata": {
    "id": "3UDqRVACt1F3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import time as time\n",
    "import warnings\n",
    "from sklearn.metrics import adjusted_rand_score, silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "warnings.filterwarnings(\"error\")"
   ],
   "metadata": {
    "id": "rLQW32fY1eTb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Im selbstgenerierten Datensatz sind die ground truth classes bekannt. Daher Werte ich das Clustering mit dem ARI aus. "
   ],
   "metadata": {
    "id": "SRfrCkIX19YA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_single_dataset(n):\n",
    "  # silhouette_scores = []\n",
    "  # calinski_h_scores = []\n",
    "  # davies_b_scores = []\n",
    "  ari = []\n",
    "\n",
    "  has_exception = []\n",
    "  iter_with_LinAlgError = []\n",
    "  iter_with_ValueError = []\n",
    "  iter_with_RuntimeWarning = []\n",
    "  iter_with_Errors_Or_Warnings = []\n",
    "  iter_without_Errors = []\n",
    "  # eigenvalues_of_one_dataset = []\n",
    "  \n",
    "  dataset_index = []\n",
    "  metric = []\n",
    "  simgraph = []\n",
    "  sigma = []\n",
    "  knn = []\n",
    "  epsi = []\n",
    "  normalized = []\n",
    "\n",
    "  for i in range(len(grid)):\n",
    "    try:\n",
    "      clust = Spectral_Clustering(X=generated_datasets[n], **grid[i], adj=False)\n",
    "    except np.linalg.LinAlgError:\n",
    "      iter_with_LinAlgError.append(i)\n",
    "      has_exception.append(1)\n",
    "      continue\n",
    "    except ValueError as e:\n",
    "      # print(e)\n",
    "      iter_with_ValueError.append(i)\n",
    "      has_exception.append(1)\n",
    "      continue\n",
    "    except RuntimeWarning as e: \n",
    "      # print(e)\n",
    "      iter_with_RuntimeWarning.append(i)\n",
    "      has_exception.append(1)\n",
    "      continue\n",
    "    except:\n",
    "      iter_with_Errors_Or_Warnings.append(i)\n",
    "      has_exception.append(1)\n",
    "      continue\n",
    "    else:\n",
    "      dataset_index.append(n)\n",
    "      has_exception.append(0)\n",
    "      metric.append(grid[i]['metric'])\n",
    "      simgraph.append(grid[i]['sim_graph'])\n",
    "      if 'knn' in grid[i]:\n",
    "        knn.append(grid[i]['knn'])\n",
    "        sigma.append(-1)\n",
    "        epsi.append(-1)\n",
    "      if 'sigma' in grid[i]:\n",
    "        sigma.append(grid[i]['sigma'])\n",
    "        knn.append(-1)\n",
    "        epsi.append(-1)\n",
    "      if 'epsi' in grid[i]:\n",
    "        epsi.append(grid[i]['epsi'])\n",
    "        knn.append(-1)\n",
    "        sigma.append(-1)\n",
    "      normalized.append(grid[i]['normalized'])\n",
    "      ari.append(adjusted_rand_score(clust.labels_, ground_truth[n]))\n",
    "\n",
    "      iter_without_Errors.append(i)\n",
    "      #score1 = silhouette_score(generated_datasets[k], clust.labels_)\n",
    "      #score2 = calinski_harabasz_score(generated_datasets[k], clust.labels_)\n",
    "      # score3 = davies_bouldin_score(generated_datasets[k], clust.labels_)\n",
    "      # silhouette_scores.append(score1)\n",
    "      # calinski_h_scores.append(score2)\n",
    "      # davies_b_scores.append(score3)\n",
    "      # eigenvalues_of_one_dataset.append(eigenvalues)\n",
    "\n",
    "      # print(f'\\t Iteration: {i}')\n",
    "      # print(f'silhouette:         {score1}')\n",
    "      # print(f'calinski_harabasz:  {score2}')\n",
    "      # print(f'davies_bouldin: \n",
    "      \n",
    "  return (# silhouette_scores, \n",
    "          # calinski_h_scores, \n",
    "          # davies_b_scores, \n",
    "\n",
    "          metric,\n",
    "          simgraph,\n",
    "          sigma,\n",
    "          knn,\n",
    "          epsi, \n",
    "          normalized,\n",
    "          \n",
    "          iter_with_LinAlgError, \n",
    "          iter_with_ValueError, \n",
    "          iter_with_RuntimeWarning, \n",
    "          iter_with_Errors_Or_Warnings, \n",
    "          iter_without_Errors, \n",
    "          # eigenvalues_of_one_dataset, \n",
    "          dataset_index,\n",
    "          ari)"
   ],
   "metadata": {
    "id": "W5Eqm2tE1fJ2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "datasets_results = {\n",
    "    # 'silhouette_score': list(),\n",
    "    # 'calinski_h_scores': list(),\n",
    "    # 'davies_b_scores': list(),\n",
    "    'ari': list(),\n",
    "    \n",
    "    'metric': list(),\n",
    "    'sim_graph': list(),\n",
    "    'sigma': list(),\n",
    "    'knn': list(),\n",
    "    'epsi': list(),\n",
    "    'normalized': list(),\n",
    "\n",
    "    'LinAlgError': list(),\n",
    "    'ValueError': list(),\n",
    "    'RuntimeWarning': list(),\n",
    "    'Errors_Or_Warnings': list(),\n",
    "    'no_Errors': list(),\n",
    "    'eigenvalues': list(),\n",
    "    'dataset_index': list(),\n",
    "}\n",
    "\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "for _ in range(3):\n",
    "  print(f'DATASET {_}')\n",
    "  (# silhouette_scores, \n",
    "   # calinski_h_scores, \n",
    "   # davies_b_scores, \n",
    "   metric, \n",
    "   simgraph,\n",
    "   sigma, \n",
    "   knn, \n",
    "   epsi, \n",
    "   normalized, \n",
    "   iter_with_LinAlgError, \n",
    "   iter_with_ValueError, \n",
    "   iter_with_RuntimeWarning, \n",
    "   iter_with_Errors_Or_Warnings, \n",
    "   iter_without_Errors, \n",
    "   # eigenvalues_of_one_dataset, \n",
    "   dataset_index, \n",
    "   ari) = evaluate_single_dataset(_)\n",
    "  # datasets_results['silhouette_score'].append(silhouette_scores)\n",
    "  # datasets_results['calinski_h_scores'].append(calinski_h_scores)\n",
    "  # datasets_results['davies_b_scores'].append(davies_b_scores)\n",
    "  \n",
    "  datasets_results['metric'].append(metric)\n",
    "  datasets_results['sim_graph'].append(simgraph)\n",
    "  datasets_results['sigma'].append(sigma)\n",
    "  datasets_results['knn'].append(knn)\n",
    "  datasets_results['epsi'].append(epsi)\n",
    "  datasets_results['normalized'].append(normalized)\n",
    "  \n",
    "  datasets_results['LinAlgError'].append(iter_with_LinAlgError)\n",
    "  datasets_results['ValueError'].append(iter_with_ValueError)\n",
    "  datasets_results['RuntimeWarning'].append(iter_with_RuntimeWarning)\n",
    "  datasets_results['Errors_Or_Warnings'].append(iter_with_Errors_Or_Warnings)\n",
    "  datasets_results['no_Errors'].append(iter_without_Errors)\n",
    "  # datasets_results['eigenvalues'].append(eigenvalues_of_one_dataset)\n",
    "  datasets_results['dataset_index'].append(dataset_index)\n",
    "  datasets_results['ari'].append(ari)\n",
    "\n",
    "executionTime = (time.time() - startTime)\n",
    "print('Execution time in seconds: ' + str(executionTime))"
   ],
   "metadata": {
    "id": "ekEuezvf1jiB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Datensatz zur Auswertung"
   ],
   "metadata": {
    "id": "3QWZz_sANr_M"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "id": "_4n42NDyNyZY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df1 = pd.DataFrame(data={\n",
    "        'Dataset': datasets_results['dataset_index'][0],\n",
    "        'Iteration': datasets_results['no_Errors'][0],\n",
    "        'metric': datasets_results['metric'][0],\n",
    "        'sim_graph': datasets_results['sim_graph'][0],\n",
    "        'sigma': datasets_results['sigma'][0],\n",
    "        'knn': datasets_results['knn'][0],\n",
    "        'epsi': np.hstack(datasets_results['epsi'][0]),#datasets_results['epsi'][0],\n",
    "        'normalised': datasets_results['normalized'][0],\n",
    "        # 'Calinski': np.hstack(datasets_results['calinski_h_scores']),\n",
    "        # 'Davis': np.hstack(datasets_results['davies_b_scores']),\n",
    "        # 'Silhouette': np.hstack(datasets_results['silhouette_score']),\n",
    "        'ari': datasets_results['ari'][0],\n",
    "        })\n",
    "df2 = pd.DataFrame(data={\n",
    "        'Dataset': datasets_results['dataset_index'][1],\n",
    "        'Iteration': datasets_results['no_Errors'][1],\n",
    "        'metric': datasets_results['metric'][1],\n",
    "        'sim_graph': datasets_results['sim_graph'][1],\n",
    "        'sigma': datasets_results['sigma'][1],\n",
    "        'knn': datasets_results['knn'][1],\n",
    "        'epsi': np.hstack(datasets_results['epsi'][1]),\n",
    "        'normalised': datasets_results['normalized'][1],\n",
    "        # 'Calinski': np.hstack(datasets_results['calinski_h_scores']),\n",
    "        # 'Davis': np.hstack(datasets_results['davies_b_scores']),\n",
    "        # 'Silhouette': np.hstack(datasets_results['silhouette_score']),\n",
    "        'ari': datasets_results['ari'][1],\n",
    "        })\n",
    "df3 = pd.DataFrame(data={\n",
    "        'Dataset': datasets_results['dataset_index'][2],\n",
    "        'Iteration': datasets_results['no_Errors'][2],\n",
    "        'metric': datasets_results['metric'][2],\n",
    "        'sim_graph': datasets_results['sim_graph'][2],\n",
    "        'sigma': datasets_results['sigma'][2],\n",
    "        'knn': datasets_results['knn'][2],\n",
    "        'epsi': np.hstack(datasets_results['epsi'][2]),\n",
    "        'normalised': datasets_results['normalized'][2],\n",
    "        # 'Calinski': np.hstack(datasets_results['calinski_h_scores']),\n",
    "        # 'Davis': np.hstack(datasets_results['davies_b_scores']),\n",
    "        # 'Silhouette': np.hstack(datasets_results['silhouette_score']),\n",
    "        'ari': datasets_results['ari'][2],\n",
    "        })\n",
    "df_final = pd.concat([df1, df2, df3], ignore_index=True, sort=False)"
   ],
   "metadata": {
    "id": "3n3r0yXYN5LQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# save Dataframe as csv in Google Drive\n",
    "from google.colab import drive\n",
    "# drive.mount('drive')\n",
    "# df_final.to_csv('Experiment_1_Test.csv')\n",
    "# Write the DataFrame to CSV file.\n",
    "with open('/content/drive/My Drive/Colab Notebooks/Bachelorarbeit/Experiment1-1_Test2.csv', 'w') as f:\n",
    "  df_final.to_csv(f)\n",
    "# !cp data.csv \"drive/folders/'Meine Ablage'/'Colab Notebooks'/Bachelorarbeit\""
   ],
   "metadata": {
    "id": "fzzJHVCcOJ18"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Datensatz importieren"
   ],
   "metadata": {
    "id": "h5zHVSmyN7aU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# from google.colab import drive\n",
    "# drive.mount('drive')\n",
    "# df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Bachelorarbeit/Experiment1-1_Test2.csv')\n",
    "df = pd.read_csv('Experiment1-1.csv')"
   ],
   "metadata": {
    "id": "WkOrRJ_JJWOo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Datensatz auf NaN-Werte überprüfen\n",
    "display(df.info())\n",
    "df.head()"
   ],
   "metadata": {
    "id": "b3ujtrB-JUSR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ARI-Werte untersuchen"
   ],
   "metadata": {
    "id": "U6bbHp0e1FeP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Skalierung der ARI-Werte untersuchen\n",
    "display(pd.unique(df.ari))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(df.ari, bins=len(pd.unique(df.ari)))\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "wyYlGOM8hiHa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.groupby('Dataset').ari.describe()"
   ],
   "metadata": {
    "id": "keHTu59TRfAT"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ARI für jeden Datensatz und jede Iteration plotten"
   ],
   "metadata": {
    "id": "pB2lmslc1N4Z"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(20.0, 10.0))\n",
    "plt.title('Adjusted Rand Index', fontsize=14)\n",
    "plt.subplot(311)\n",
    "plt.title('Datensatz 1')\n",
    "aris = df[df.Dataset == 0].ari\n",
    "plt.plot(aris, 'bo', aris, 'k')\n",
    "plt.xticks(np.arange(int(aris.index[0]), int(aris.index[len(aris)-1]), 20))\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.title('Datensatz 2')\n",
    "aris = df[df.Dataset == 1].ari\n",
    "plt.plot(aris, 'bo', aris, 'k')\n",
    "plt.xticks(np.arange(int(aris.index[0]), int(aris.index[len(aris)-1]), 20))\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.title('Datensatz 3')\n",
    "aris = df[df.Dataset == 2].ari\n",
    "plt.plot(aris, 'bo', aris, 'k')\n",
    "plt.xticks(np.arange(int(aris.index[0]), int(aris.index[len(aris)-1]), 20))\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "NdrQwXdxDmy7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## *1.* Untersuchung der Metrik auf die Clusterqualität\n",
    "\n",
    "Ich betrachte, bei welcher Metrik eine höhere Clustergüte erreicht wird. Dazu betrachte ich eine deskriptive Statistik aller Datensätze zusammen und der einzelnen Datensätze.\n",
    "Ich untersuche erst jeden einzelnen Datensatz und anschließend alle zusammen."
   ],
   "metadata": {
    "id": "ci5GKRI3acfd"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.groupby('metric').ari.describe().round(4)"
   ],
   "metadata": {
    "id": "GlMf41qnoiT1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.groupby([ 'Dataset', 'metric']).ari.describe().round(4)"
   ],
   "metadata": {
    "id": "Hj-vkW9AoiT1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## *2.* Einfluss des Ähnlichkeitsgraphen auf die Clusterqualität"
   ],
   "metadata": {
    "collapsed": false,
    "id": "StKDfiluoiT2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.groupby('sim_graph').ari.describe()"
   ],
   "metadata": {
    "id": "KUbcC2WSoiT2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.groupby([ 'Dataset', 'sim_graph']).ari.describe().round(4)"
   ],
   "metadata": {
    "id": "vod1DrH1RfAW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df[df.sim_graph == 'fully_connect'].groupby([ 'Dataset', 'metric']).ari.describe().round(4)"
   ],
   "metadata": {
    "id": "vltNQclMQLwj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# *3.* Einfluss von $\\sigma$ auf die Clusterqualität\n",
    "Der Einfluss soll mit der Korrelation berechnet werden. Um einzuschätzen, welches Korrelationsmaß verwendet werden soll und damit ich eine Einschätzung über die Richtung der Korrelation habe, plotte ich sigma und die Clustergüte in einem Scatterplot.\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "_u7RPwPvoiT3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ],
   "metadata": {
    "id": "L7H7HQwjRfAY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.scatterplot(data=df[df.sigma != -1], x='sigma', y='ari')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "5V2xxL7MoiT3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[df.sigma != -1].sigma.corr(df[df.sigma != -1].ari, method='spearman')"
   ],
   "metadata": {
    "id": "YK789m5JoiT4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Was sind die häufigsten Werte? \n",
    "ari_round_unique = np.unique(np.round(df[df.sigma != 1].ari,2), return_counts=True)\n",
    "# sortiere ari_round nach der Häufigkeit\n",
    "ari_round_unique_sorted = np.array([x for _,x in sorted(zip(ari_round_unique[1],ari_round_unique[0]), reverse=True)])\n",
    "ari_round_unique_sorted"
   ],
   "metadata": {
    "id": "Wk-6CmRBBK0A"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df2 = df[df.sigma != -1].copy()\n",
    "df2.ari = np.round(df2.ari, 2)\n",
    "# eine Kopie von df2 mit den ersten 4 Sigma-Werten von ari_round_unique_sorted erstellen\n",
    "df3 = df2[df2.ari.isin(ari_round_unique_sorted[:4])].copy()\n",
    "# df3.groupby(['ari', 'Dataset','normalised', 'metric']).ari.describe()\n",
    "# df3.groupby(['ari', 'metric']).sigma.describe()\n",
    "# df3.groupby(['sigma']).ari.describe()\n",
    "df3.groupby(['Dataset', 'ari']).sigma.describe().round(4)"
   ],
   "metadata": {
    "id": "BCCznUj29hnY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df[df.Dataset == 0].shape"
   ],
   "metadata": {
    "id": "WbNJZV2laJvi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# *4.1* Einfluss von $k$ beim knn-Graphen auf die Clusterqualität"
   ],
   "metadata": {
    "collapsed": false,
    "id": "e2OHYbG5oiT4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plotte die Spalte k, bei dem der sim-graph knn ist und die Clustergüte in einem Scatterplot\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.scatterplot(data=df[df.sim_graph == 'knn'], x='knn', y='ari')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "67dM2emroiT5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# *4.2* Einfluss von $k$ beim mutual-knn-Graphen auf die Clusterqualität"
   ],
   "metadata": {
    "collapsed": false,
    "id": "B0q5rCWhRfAa"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.scatterplot(data=df[df.sim_graph == 'mutual_knn'], x='knn', y='ari')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "Yw_uMWrbRfAa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Wie viele Iterationen wurden insgesammt mit dem mutual-knn erstellt.\n",
    "df[df.sim_graph == 'mutual_knn'] \n",
    "# df[(df.sim_graph == 'mutual_knn') & (df.ari != 1.0) & (df.ari != 0.0)]"
   ],
   "metadata": {
    "id": "FfSwykqiduPp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df[(df.normalised == 2) & (df.sim_graph == 'mutual_knn')]"
   ],
   "metadata": {
    "id": "MZ6Mu2bRxXic"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# *5.* Einfluss von $\\epsilon$ auf die Clusterqualität"
   ],
   "metadata": {
    "collapsed": false,
    "id": "KH7FXvmZoiT5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(data=df[df.sim_graph == 'eps_neighbor'], x='epsi', y='ari')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "8izUu0uNoiT5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df[df.sim_graph == 'eps_neighbor']"
   ],
   "metadata": {
    "id": "6P-aJFNu0L4E"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ab einem bestimmten epsi-Wert nimmt die Clustergüte ab. Ich filtere die epsi-Werte heraus, dei dem der ARI-Wert größer 0.8 ist."
   ],
   "metadata": {
    "collapsed": false,
    "id": "mU1Y0lsfRfAb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# return epsi values with ari >= 0.8 and ari <= 0.8\n",
    "epsi_ari_better_model = df[(df.ari >= 0.8) & (df.epsi != -1)].epsi.unique()\n",
    "display(pd.DataFrame(epsi_ari_better_model).describe())\n",
    "epsi_ari_worse_model = df[(df.ari < 0.8) & (df.epsi != -1)].epsi.unique()\n",
    "display(pd.DataFrame(epsi_ari_worse_model).describe())"
   ],
   "metadata": {
    "id": "CYY-f4D69hnb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[df.epsi != -1].epsi.corr(df[df.epsi != -1].ari, method='spearman')"
   ],
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "id": "Fa_FG2tE9hnb"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# *6.* Einfluss vom Laplace-Graphen auf die Clusterqualität"
   ],
   "metadata": {
    "collapsed": false,
    "id": "ksP31AwaRfAb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.groupby('normalised').ari.describe().round(4)"
   ],
   "metadata": {
    "id": "g4Egs6YzoiT6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.groupby(['Dataset', 'normalised']).ari.describe().round(4)"
   ],
   "metadata": {
    "id": "Qj1kkdQJRfAc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(, 5))\n",
    "sns.boxplot(data=df, x='normalised', y='ari', hue='Dataset')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "NNNPnPymMiZy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Zusammenfassung"
   ],
   "metadata": {
    "id": "07vemVP8ZCCG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "a = df.groupby([ 'Dataset', 'metric']).ari.describe().round(4)\n",
    "b = df.groupby([ 'Dataset', 'sim_graph']).ari.describe().round(4)\n",
    "c = df[df.sim_graph == 'fully_connect'].groupby([ 'Dataset', 'metric']).ari.describe().round(4)\n",
    "c.index = pd.MultiIndex.from_tuples([(x[0], \"fully_\" + x[1]) for x in c.index])\n",
    "d = df.groupby(['Dataset', 'normalised']).ari.describe().round(4)\n",
    "\n",
    "frames = [a,b,c,d]\n",
    "summary = pd.concat(frames)\n",
    "summary = summary.rename({\"metric\":\"parameter\"})"
   ],
   "metadata": {
    "id": "KLxuA5UJYtFR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "                           count    mean     std\nDataset metric                                  \n0       fully_euclidean     90.0  1.0000  0.0000\n        fully_connect      180.0  0.9607  0.0394\n        fully_correlation   90.0  0.9213  0.0000\n        knn                102.0  0.9080  0.1985\n        correlation        182.0  0.8201  0.2888\n        2                  145.0  0.7872  0.3577\n        1                  149.0  0.7745  0.3758\n        euclidean          278.0  0.6965  0.4460\n        3                  166.0  0.6828  0.4371\n2       knn                102.0  0.6261  0.3220\n        euclidean          279.0  0.6073  0.3491\n        mutual_knn          65.0  0.5776  0.4939\n        eps_neighbor       114.0  0.5522  0.2542\n        2                  146.0  0.5330  0.2943\n        1                  149.0  0.5116  0.3038\n0       mutual_knn          64.0  0.4688  0.5030\n2       3                  166.0  0.4578  0.3299\n1       knn                102.0  0.4417  0.4987\n0       eps_neighbor       114.0  0.4154  0.4544\n2       fully_correlation   90.0  0.3741  0.0000\n        fully_connect      180.0  0.3649  0.1778\n        fully_euclidean     90.0  0.3557  0.2518\n        correlation        182.0  0.3330  0.1173\n1       mutual_knn          64.0  0.3125  0.4672\n        euclidean          278.0  0.3017  0.4317\n        2                  136.0  0.2266  0.4113\n        1                  149.0  0.1689  0.3662\n        3                  166.0  0.1650  0.3347\n        fully_euclidean     90.0  0.1015  0.2338\n        eps_neighbor       114.0  0.0829  0.2169\n        fully_connect      171.0  0.0519  0.1771\n        correlation        173.0 -0.0028  0.0011\n        fully_correlation   81.0 -0.0032  0.0002",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n    </tr>\n    <tr>\n      <th>Dataset</th>\n      <th>metric</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"9\" valign=\"top\">0</th>\n      <th>fully_euclidean</th>\n      <td>90.0</td>\n      <td>1.0000</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>fully_connect</th>\n      <td>180.0</td>\n      <td>0.9607</td>\n      <td>0.0394</td>\n    </tr>\n    <tr>\n      <th>fully_correlation</th>\n      <td>90.0</td>\n      <td>0.9213</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>knn</th>\n      <td>102.0</td>\n      <td>0.9080</td>\n      <td>0.1985</td>\n    </tr>\n    <tr>\n      <th>correlation</th>\n      <td>182.0</td>\n      <td>0.8201</td>\n      <td>0.2888</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>145.0</td>\n      <td>0.7872</td>\n      <td>0.3577</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>149.0</td>\n      <td>0.7745</td>\n      <td>0.3758</td>\n    </tr>\n    <tr>\n      <th>euclidean</th>\n      <td>278.0</td>\n      <td>0.6965</td>\n      <td>0.4460</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>166.0</td>\n      <td>0.6828</td>\n      <td>0.4371</td>\n    </tr>\n    <tr>\n      <th rowspan=\"6\" valign=\"top\">2</th>\n      <th>knn</th>\n      <td>102.0</td>\n      <td>0.6261</td>\n      <td>0.3220</td>\n    </tr>\n    <tr>\n      <th>euclidean</th>\n      <td>279.0</td>\n      <td>0.6073</td>\n      <td>0.3491</td>\n    </tr>\n    <tr>\n      <th>mutual_knn</th>\n      <td>65.0</td>\n      <td>0.5776</td>\n      <td>0.4939</td>\n    </tr>\n    <tr>\n      <th>eps_neighbor</th>\n      <td>114.0</td>\n      <td>0.5522</td>\n      <td>0.2542</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>146.0</td>\n      <td>0.5330</td>\n      <td>0.2943</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>149.0</td>\n      <td>0.5116</td>\n      <td>0.3038</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <th>mutual_knn</th>\n      <td>64.0</td>\n      <td>0.4688</td>\n      <td>0.5030</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <th>3</th>\n      <td>166.0</td>\n      <td>0.4578</td>\n      <td>0.3299</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <th>knn</th>\n      <td>102.0</td>\n      <td>0.4417</td>\n      <td>0.4987</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <th>eps_neighbor</th>\n      <td>114.0</td>\n      <td>0.4154</td>\n      <td>0.4544</td>\n    </tr>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">2</th>\n      <th>fully_correlation</th>\n      <td>90.0</td>\n      <td>0.3741</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>fully_connect</th>\n      <td>180.0</td>\n      <td>0.3649</td>\n      <td>0.1778</td>\n    </tr>\n    <tr>\n      <th>fully_euclidean</th>\n      <td>90.0</td>\n      <td>0.3557</td>\n      <td>0.2518</td>\n    </tr>\n    <tr>\n      <th>correlation</th>\n      <td>182.0</td>\n      <td>0.3330</td>\n      <td>0.1173</td>\n    </tr>\n    <tr>\n      <th rowspan=\"10\" valign=\"top\">1</th>\n      <th>mutual_knn</th>\n      <td>64.0</td>\n      <td>0.3125</td>\n      <td>0.4672</td>\n    </tr>\n    <tr>\n      <th>euclidean</th>\n      <td>278.0</td>\n      <td>0.3017</td>\n      <td>0.4317</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>136.0</td>\n      <td>0.2266</td>\n      <td>0.4113</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>149.0</td>\n      <td>0.1689</td>\n      <td>0.3662</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>166.0</td>\n      <td>0.1650</td>\n      <td>0.3347</td>\n    </tr>\n    <tr>\n      <th>fully_euclidean</th>\n      <td>90.0</td>\n      <td>0.1015</td>\n      <td>0.2338</td>\n    </tr>\n    <tr>\n      <th>eps_neighbor</th>\n      <td>114.0</td>\n      <td>0.0829</td>\n      <td>0.2169</td>\n    </tr>\n    <tr>\n      <th>fully_connect</th>\n      <td>171.0</td>\n      <td>0.0519</td>\n      <td>0.1771</td>\n    </tr>\n    <tr>\n      <th>correlation</th>\n      <td>173.0</td>\n      <td>-0.0028</td>\n      <td>0.0011</td>\n    </tr>\n    <tr>\n      <th>fully_correlation</th>\n      <td>81.0</td>\n      <td>-0.0032</td>\n      <td>0.0002</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.sort_values(by=['mean','std'], ascending=[False, True])[['count', 'mean', 'std']]"
   ],
   "metadata": {
    "id": "Tha9271_umTU",
    "outputId": "c48fd5bc-8def-4e18-bcfd-1f4f5744fcaa"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "                                              count    mean     std\nDataset sim_graph     metric      normalised                       \n0       fully_connect euclidean   1            30.0  1.0000  0.0000\n                                  2            30.0  1.0000  0.0000\n                                  3            30.0  1.0000  0.0000\n        knn           euclidean   1            17.0  0.9412  0.2425\n                                  3            17.0  0.9238  0.2205\n...                                             ...     ...     ...\n2       fully_connect euclidean   3            30.0  0.3406  0.2620\n        eps_neighbor  correlation 2             8.0  0.3279  0.1308\n                                  1             8.0  0.3273  0.1323\n                                  3             8.0  0.3272  0.1326\n        mutual_knn    correlation 3            17.0  0.0000  0.0000\n\n[66 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n    </tr>\n    <tr>\n      <th>Dataset</th>\n      <th>sim_graph</th>\n      <th>metric</th>\n      <th>normalised</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">0</th>\n      <th rowspan=\"3\" valign=\"top\">fully_connect</th>\n      <th rowspan=\"3\" valign=\"top\">euclidean</th>\n      <th>1</th>\n      <td>30.0</td>\n      <td>1.0000</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>30.0</td>\n      <td>1.0000</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>30.0</td>\n      <td>1.0000</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">knn</th>\n      <th rowspan=\"2\" valign=\"top\">euclidean</th>\n      <th>1</th>\n      <td>17.0</td>\n      <td>0.9412</td>\n      <td>0.2425</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>17.0</td>\n      <td>0.9238</td>\n      <td>0.2205</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">2</th>\n      <th>fully_connect</th>\n      <th>euclidean</th>\n      <th>3</th>\n      <td>30.0</td>\n      <td>0.3406</td>\n      <td>0.2620</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">eps_neighbor</th>\n      <th rowspan=\"3\" valign=\"top\">correlation</th>\n      <th>2</th>\n      <td>8.0</td>\n      <td>0.3279</td>\n      <td>0.1308</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8.0</td>\n      <td>0.3273</td>\n      <td>0.1323</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8.0</td>\n      <td>0.3272</td>\n      <td>0.1326</td>\n    </tr>\n    <tr>\n      <th>mutual_knn</th>\n      <th>correlation</th>\n      <th>3</th>\n      <td>17.0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n    </tr>\n  </tbody>\n</table>\n<p>66 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['Dataset', 'sim_graph', 'metric', 'normalised']).ari.describe().round(4).sort_values(by=['Dataset','mean','std'], ascending=[True, False, True])[['count','mean', 'std']]"
   ],
   "metadata": {
    "id": "gD9PTe5sumTU",
    "outputId": "5e4ea86a-2297-4f00-cdd7-521eb3dec742"
   }
  }
 ]
}
